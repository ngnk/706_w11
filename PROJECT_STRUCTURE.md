# PySpark Pipeline - Project Structure

```
pyspark-pipeline/
â”‚
â”œâ”€â”€ ğŸ““ pyspark_pipeline.ipynb          # Main Jupyter notebook
â”œâ”€â”€ ğŸ pyspark_pipeline_script.py     # Standalone Python script version
â”‚
â”œâ”€â”€ ğŸ“š README.md                       # Comprehensive documentation
â”œâ”€â”€ âš¡ QUICKSTART.md                   # Quick setup guide
â”œâ”€â”€ ğŸ“‹ requirements.txt                # Python dependencies
â”œâ”€â”€ ğŸ”§ download_data.sh               # Data download helper script
â”œâ”€â”€ ğŸ“ PROJECT_STRUCTURE.md           # This file
â”œâ”€â”€ ğŸš« .gitignore                      # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“Š screenshots/                    # Screenshots for submission
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â”œâ”€â”€ execution_plan.png            # (Add after running)
â”‚   â”œâ”€â”€ spark_ui_jobs.png             # (Add after running)
â”‚   â”œâ”€â”€ spark_ui_sql.png              # (Add after running)
â”‚   â”œâ”€â”€ query_details.png             # (Add after running)
â”‚   â”œâ”€â”€ pipeline_success.png          # (Add after running)
â”‚   â””â”€â”€ caching_performance.png       # (Add after running)
â”‚
â”œâ”€â”€ ğŸ“ outputs/                        # Pipeline outputs
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â”œâ”€â”€ hourly_stats.parquet/         # (Generated by pipeline)
â”‚   â”œâ”€â”€ passenger_stats.parquet/      # (Generated by pipeline)
â”‚   â”œâ”€â”€ weekend_comparison.parquet/   # (Generated by pipeline)
â”‚   â”œâ”€â”€ processed_trips.parquet/      # (Generated by pipeline)
â”‚   â””â”€â”€ taxi_analysis_dashboard.png   # (Generated by pipeline)
â”‚
â””â”€â”€ ğŸ’¾ data/                           # Input data (not in git)
    â”œâ”€â”€ .gitkeep
    â”œâ”€â”€ yellow_tripdata_2023-01.parquet  # (Download or generate)
    â”œâ”€â”€ yellow_tripdata_2023-02.parquet  # (Optional)
    â””â”€â”€ yellow_tripdata_2023-03.parquet  # (Optional)
```

---

## ğŸ“ File Descriptions

### Core Files

**pyspark_pipeline.ipynb**
- Main assignment deliverable
- Interactive Jupyter notebook with all required components
- Includes documentation, code, and outputs
- Best for step-by-step execution and learning

**pyspark_pipeline_script.py**
- Standalone Python script version
- Same functionality as notebook
- Better for automated execution
- Can be run with `spark-submit`

### Documentation

**README.md** (Most Important!)
- Comprehensive project documentation
- Dataset description and source
- Performance analysis (2-3 paragraphs)
- Key findings from data analysis
- Optimization strategies explained
- Screenshot locations and descriptions

**QUICKSTART.md**
- Fast setup guide (5 minutes)
- Troubleshooting tips
- Screenshot checklist
- Expected execution times

**PROJECT_STRUCTURE.md** (This file)
- Project organization
- File purposes
- Usage instructions

### Supporting Files

**requirements.txt**
- Python package dependencies
- PySpark, pandas, matplotlib, etc.
- Install with: `pip install -r requirements.txt`

**download_data.sh**
- Helper script to download NYC taxi data
- Downloads 2.1GB of real data
- Make executable: `chmod +x download_data.sh`
- Run: `./download_data.sh`

**.gitignore**
- Excludes large data files from git
- Excludes Python cache and logs
- Keeps project clean

---

## ğŸ¯ Assignment Requirements Checklist

### Required Components

- [x] **Load 1GB+ dataset**: NYC Taxi data or synthetic
- [x] **2+ filter operations**: Data quality + range filters
- [x] **1+ join operation**: Broadcast join with zone lookup
- [x] **1+ groupBy aggregation**: Multiple aggregations implemented
- [x] **Column transformations**: 7+ withColumn operations
- [x] **2+ SQL queries**: Hourly stats + weekend comparison
- [x] **Query optimization**: Filter pushdown, column pruning, partitioning
- [x] **Write to Parquet**: Multiple output files
- [x] **Performance analysis**: Documented in README
- [x] **.explain() output**: Execution plan analysis
- [x] **Lazy vs Eager demo**: Clear demonstration included

### Bonus Components

- [x] **Caching optimization**: Performance comparison with metrics
- [x] **Broadcast join**: Optimized join implementation
- [x] **Synthetic data generator**: For easy testing
- [x] **Visualizations**: Dashboard with 4 charts

---

## ğŸš€ Usage Instructions

### 1. Initial Setup
```bash
# Clone/download project
cd pyspark-pipeline

# Setup environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Get Data

**Option A: Download real data (2.1GB)**
```bash
./download_data.sh
```

**Option B: Use synthetic data**
- Already included in notebook
- Automatically generates 10M records
- No download needed

### 3. Run Pipeline

**Jupyter Notebook (Recommended)**
```bash
jupyter notebook pyspark_pipeline.ipynb
# Run all cells
# Keep Spark UI open for screenshots
```

**Python Script**
```bash
python pyspark_pipeline_script.py
# Check outputs/ directory for results
```

### 4. Capture Screenshots

While pipeline is running:
1. Open http://localhost:4040 (Spark UI)
2. Navigate to SQL, Jobs, and Stages tabs
3. Take screenshots as listed in README.md
4. Save to screenshots/ directory

### 5. Export Notebook (for submission)

```bash
# Export to HTML
jupyter nbconvert --to html pyspark_pipeline.ipynb

# This creates: pyspark_pipeline.html
```

---

## ğŸ“¦ What to Submit

### GitHub Repository should contain:

1. **README.md** (with your analysis and screenshots embedded)
2. **pyspark_pipeline.ipynb** (main notebook)
3. **pyspark_pipeline.html** (exported HTML version)
4. **requirements.txt**
5. **screenshots/** directory with all required images
6. **.gitignore** (to keep repo clean)

### Do NOT include:
- data/ directory (too large)
- outputs/ directory (generated files)
- venv/ directory (Python environment)

---

## ğŸ“Š Output Files Generated

After running the pipeline:

### Parquet Files (in outputs/)
- `hourly_stats.parquet/` - 24 hours of aggregated metrics
- `passenger_stats.parquet/` - Analysis by passenger count
- `weekend_comparison.parquet/` - Weekend vs weekday patterns
- `processed_trips.parquet/` - Sample of 100K processed records

### Visualizations
- `taxi_analysis_dashboard.png` - 4-chart dashboard

### Size Reference
- Each parquet directory: 1-10 KB (aggregated data)
- Dashboard PNG: ~200 KB
- Total outputs: < 1 MB

---

## ğŸ”§ Customization Options

### Adjust Dataset Size
```python
# In notebook/script
df_raw = generate_synthetic_taxi_data(5_000_000)  # 5M records
```

### Change Output Location
```python
output_path = "/path/to/outputs/"
```

### Modify Spark Configuration
```python
spark = SparkSession.builder \
    .config("spark.executor.memory", "8g") \
    .config("spark.sql.shuffle.partitions", "400") \
    .getOrCreate()
```

### Use Different Data
```python
# Load your own Parquet files
data_path = "path/to/your/data/*.parquet"
df_raw = spark.read.parquet(data_path)
```

---

## ğŸ“ Learning Outcomes

This project demonstrates:

1. **Distributed Data Processing**
   - Handling 1GB+ datasets
   - Understanding partitioning
   - Parallel computation

2. **Query Optimization**
   - Filter pushdown
   - Column pruning
   - Broadcast joins
   - Appropriate partitioning

3. **PySpark Fundamentals**
   - Lazy evaluation
   - Transformations vs actions
   - DataFrame API
   - Spark SQL

4. **Performance Analysis**
   - Reading execution plans
   - Identifying bottlenecks
   - Measuring improvements
   - Caching strategies

---

## ğŸ’¡ Tips for Success

1. **Run the pipeline completely** before taking screenshots
2. **Keep Spark UI open** during execution
3. **Read the execution plans** to understand optimizations
4. **Document your observations** in README.md
5. **Test with synthetic data first** before downloading large files
6. **Export notebook to HTML** for submission

---

## ğŸ†˜ Getting Help

If you encounter issues:

1. Check QUICKSTART.md for common problems
2. Review Spark UI for error messages
3. Check `derby.log` for detailed errors
4. Ensure sufficient memory (8GB+ recommended)
5. Try reducing dataset size for testing

---

## âœ… Pre-Submission Checklist

- [ ] Notebook runs without errors
- [ ] All cells executed in order
- [ ] Screenshots captured and saved
- [ ] README.md updated with analysis
- [ ] Notebook exported to HTML
- [ ] GitHub repository created
- [ ] All required files pushed to GitHub
- [ ] .gitignore properly configured
- [ ] Repository is public (or shared with instructor)

---

## ğŸ‰ You're All Set!

Your PySpark pipeline is ready for Week 11 assignment submission.

Good luck! ğŸš€
